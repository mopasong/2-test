# ğŸŒ HuggingFace API ê¸°ë°˜ GPT ëŒ€í™” ì˜ˆì‹œ (Colab ì‹¤í–‰ìš©)
# ëª¨ë¸: bigscience/bloomz-560m (Inference API ì‚¬ìš©)

!pip install -q requests

import requests

# âœ… HuggingFace API í† í° ì…ë ¥ (ë§¨ ì²˜ìŒ í•œ ë²ˆë§Œ)
HUGGINGFACE_TOKEN = "hf_your_token_here"  # â† ì—¬ê¸°ì— ë³¸ì¸ì˜ í† í° ì…ë ¥í•˜ì„¸ìš”
MODEL = "bigscience/bloomz-560m"

def call_huggingface_api(prompt, token=HUGGINGFACE_TOKEN, model=MODEL):
    url = f"https://api-inference.huggingface.co/models/{model}"
    headers = {"Authorization": f"Bearer {token}"}
    payload = {
        "inputs": prompt,
        "parameters": {
            "temperature": 0.7,
            "max_new_tokens": 150
        }
    }

    response = requests.post(url, headers=headers, json=payload)
    if response.status_code == 200:
        result = response.json()
        if isinstance(result, list):
            return result[0].get("generated_text", str(result[0]))
        return str(result)
    else:
        return f"[âŒ Error {response.status_code}] {response.text}"

# ğŸ§ª í…ŒìŠ¤íŠ¸
prompt = "ì™œ ì‚¬ëŒì€ ë°˜ë³µí•´ì„œ ê°™ì€ ê°ì •ì„ ëŠë‚„ê¹Œìš”?"
print("ğŸ¤– GPT ì‘ë‹µ:", call_huggingface_api(prompt))
